{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing IPython cluster clients and printing their ids to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n",
      "FINSHED\n"
     ]
    }
   ],
   "source": [
    "import ipyparallel as parallel\n",
    "\n",
    "clients = parallel.Client()\n",
    "clients.block = True  # use synchronous computations\n",
    "print (clients.ids)\n",
    "print (\"FINSHED\")\n",
    "dv = clients[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing mpi4py and numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "from mpi4py import MPI\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing Convolution, you don't need to modify this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "def convolve_func(main,kernel,KERNEL_DIM,DIMx,DIMy,upper_pad,lower_pad):\n",
    "\tnum_pads = int((KERNEL_DIM - 1) / 2)\n",
    "\tconv = np.zeros(main.shape,dtype=int)\n",
    "\tmain = np.concatenate((upper_pad,main,lower_pad))\n",
    "\tfor i in range(DIMy):\n",
    "\t\tfor j in range(DIMx):\n",
    "\t\t\tfor k in range(KERNEL_DIM):\n",
    "\t\t\t\tfor l in range(KERNEL_DIM):\n",
    "\t\t\t\t\tif j+l <= DIMx+1 and i+k>=num_pads and i+k<=DIMy:\n",
    "\t\t\t\t\t\tconv[j*DIMy+i] += main[(j+l)*DIMy+i-num_pads+k]#*kernel[k][l]\n",
    "\treturn conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 points: \n",
    "Load MPI communicator, get the total number of processes and rank of the process                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 points: \n",
    "Load or initialize data array and kernel array only in process 0(rank 0)                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "DIMx = 0\n",
    "DIMy = 0\n",
    "KERNEL_DIM = 0\n",
    "\n",
    "#Add a condition such that these intializations below should happen in only process 0\n",
    "if rank == 0:\n",
    "    img = np.array([[3, 9, 5, 9],[1, 7, 4, 3],[2, 1, 6, 5],\n",
    "                    [3, 9, 5, 9],[1, 7, 4, 3],[2, 1, 6, 5],\n",
    "                    [3, 9, 5, 9],[1, 7, 4, 3],[2, 1, 6, 5]])\n",
    "    kernel = np.array([[0, 1, 0],[0, 0, 0],[0, -1, 0]])\n",
    "    DIMx = img.shape[0]\n",
    "    DIMy = img.shape[1]\n",
    "    KERNEL_DIM = int(kernel.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 points: \n",
    "Broadcast data and kernel array sizes from process 0 to  all other processes                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "#broadcast data and kernel array sizes (think why we are broadcasting sizes)\n",
    "DIMx = comm.bcast(DIMx, root=0)\n",
    "DIMy = comm.bcast(DIMy, root=0)\n",
    "KERNEL_DIM = comm.bcast(KERNEL_DIM, root=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize empty kernel array for all  processes except rank = 0, why we are not initialzing kernel array for rank 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:The kernel array is already initalized for rank 0 from a previous node. If I initialized it here the values would be overwritten, removing the real data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "#initialize empty kernel array except for process 0(rank=0)\n",
    "if rank != 0:\n",
    "    kernel = np.empty((KERNEL_DIM, KERNEL_DIM), dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 points: \n",
    "Broadcast Kernel array from rank 0 to all other processes.                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "#broadcast kernel array from rank 0 to all other processes\n",
    "kernel = comm.bcast(kernel, root=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25 points: \n",
    "Split the rows in data array equally and scatter them from process 0 to all other process. To split them \n",
    "equally, number of rows in the data array must be a integral multiple of number of processes. MPI has ways \n",
    "to send unequal chunks of data between processses. But for here you can do with equal number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "#split and send data array to corresponding processses (you need to initialize a buffer to receive data from \n",
    "#process 0, similar to the random initializing done for kernel array)\n",
    "\n",
    "rows = DIMx // size\n",
    "\n",
    "localImg = np.empty((rows, DIMy), dtype=int)\n",
    "\n",
    "if rank == 0:\n",
    "    comm.Scatter(img, localImg, root=0)\n",
    "else:\n",
    "    comm.Scatter(None, localImg, root=0)\n",
    "#Here does we initialize buffer for process 0 also, if so why?(Hint: because of the function we are using to send \n",
    "#and receieve data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25 points: \n",
    "For convolution of kernel array and data array, you have to pass the kernel padding rows from one\n",
    "process to another. please see objective for more details. Send and Recieve rows from one process \n",
    "to other. Careful with the data size and tags you are sending and receiving should match otherwise\n",
    "commincator will wait for them indefintely.                                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "#send padding rows from one process to other (carefully observe which process to send data to which process and\n",
    "# which process receives the data)\n",
    "\n",
    "upperPad = np.zeros(DIMy, dtype=int)\n",
    "lowerPad = np.zeros(DIMy, dtype=int)\n",
    "\n",
    "if rank == 0:\n",
    "    comm.send(localImg[-1, :], dest=1, tag=0)\n",
    "    lowerPad = comm.recv(source=1, tag=1)\n",
    "\n",
    "elif rank == 1:\n",
    "    upperPad = comm.recv(source=0, tag=0)\n",
    "    comm.send(localImg[0, :], dest=0, tag=1)\n",
    "\n",
    "    comm.send(localImg[-1, :], dest=2, tag=2)\n",
    "    lowerPad = comm.recv(source=2, tag=3)\n",
    "\n",
    "elif rank == 2:\n",
    "    upperPad = comm.recv(source=1, tag=2)\n",
    "    comm.send(localImg[0, :], dest=1, tag=3)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why we are loading data into process 0 and broadcasting input data to all other processes? are there any other methods to load data into all processes (not for evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: The scatter operation requires a source array to split from and process 0 holds the original data. We could use Bcast to send the data but this would send the entire array instead of just the smaller, strictly needed data which saves resources. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 points: \n",
    "Perform Convolution operation by calling convolve_func() provided for each of the process with \n",
    "corresponding rows as arguments.                                                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "#convolution function arguments\n",
    "#main - data array (flattened array), only the part of the data array that is processed for each process\n",
    "#kernel - kernel array\n",
    "#DIMy - ColumnSize\n",
    "#Dimx - RowSize\n",
    "#upper_pad = upper padding row\n",
    "#lower_pad = lower padding row\n",
    "main = localImg.flatten()\n",
    "\n",
    "convResult = convolve_func(main, kernel, KERNEL_DIM, rows, DIMy, upperPad, lowerPad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 points: \n",
    "Gather the computed convolutional matrix rows to process 0.                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "#To receive data from all processes, process 0 should have a buffer\n",
    "if rank == 0:\n",
    "    recvbuf = np.empty((DIMx * DIMy), dtype=int)\n",
    "else:\n",
    "    recvbuf = None\n",
    "\n",
    "comm.Gather(convResult, recvbuf, root=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape the flattened array to match input dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "#Reshape the collected array to the input image dimensions\n",
    "if rank == 0:\n",
    "    recvbuf = recvbuf.reshape(DIMx, DIMy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 points: \n",
    "Test to check sequential convolution and MPI based parallel convolution outputs                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare result: True\n"
     ]
    }
   ],
   "source": [
    "#I could not resolve the LookupError that was occuring when trying to do the parallel \n",
    "#execution so I added this instead using zero padding instead of upper/lower. \n",
    "#Based on this output I believe I correctly implemented the lab.\n",
    "\n",
    "\n",
    "def _compare_on_rank0():\n",
    "    try:\n",
    "        main_grid = img.flatten()\n",
    "        conv1 = convolve_func(main_grid, kernel, KERNEL_DIM, DIMx, DIMy, np.zeros(DIMy, dtype=int), np.zeros(DIMy, dtype=int))\n",
    "        conv1 = conv1.reshape(DIMx, DIMy)\n",
    "        return np.array_equal(conv1, recvbuf)\n",
    "    except Exception as e:\n",
    "        return f\"error: {e}\"\n",
    "\n",
    "result = clients[0].apply_sync(_compare_on_rank0)\n",
    "print(\"Compare result:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in callback BaseAsyncIOLoop._handle_events()\n",
      "handle: <Handle BaseAsyncIOLoop._handle_events()>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\joeno\\anaconda3\\envs\\myenv\\Lib\\asyncio\\events.py\", line 89, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\joeno\\anaconda3\\envs\\myenv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 208, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\joeno\\anaconda3\\envs\\myenv\\Lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 600, in _handle_events\n",
      "    self._handle_recv()\n",
      "    ~~~~~~~~~~~~~~~~~^^\n",
      "  File \"C:\\Users\\joeno\\anaconda3\\envs\\myenv\\Lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 629, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "    ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\joeno\\anaconda3\\envs\\myenv\\Lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 550, in _run_callback\n",
      "    f = callback(*args, **kwargs)\n",
      "  File \"C:\\Users\\joeno\\anaconda3\\envs\\myenv\\Lib\\site-packages\\decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"C:\\Users\\joeno\\anaconda3\\envs\\myenv\\Lib\\site-packages\\ipyparallel\\client\\client.py\", line 76, in unpack_message\n",
      "    return f(self, msg)\n",
      "  File \"C:\\Users\\joeno\\anaconda3\\envs\\myenv\\Lib\\site-packages\\ipyparallel\\client\\client.py\", line 1225, in _dispatch_iopub\n",
      "    callback(msg)\n",
      "    ~~~~~~~~^^^^^\n",
      "  File \"C:\\Users\\joeno\\anaconda3\\envs\\myenv\\Lib\\site-packages\\ipyparallel\\client\\asyncresult.py\", line 166, in _iopub_streaming_output_callback\n",
      "    publish_display_data(\n",
      "    ~~~~~~~~~~~~~~~~~~~~^\n",
      "        {\n",
      "        ^\n",
      "    ...<3 lines>...\n",
      "        update=update,\n",
      "        ^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\joeno\\anaconda3\\envs\\myenv\\Lib\\site-packages\\IPython\\core\\display_functions.py\", line 73, in publish_display_data\n",
      "    display_pub.publish(\n",
      "    ~~~~~~~~~~~~~~~~~~~^\n",
      "        data=data,\n",
      "        ^^^^^^^^^^\n",
      "        metadata=metadata,\n",
      "        ^^^^^^^^^^^^^^^^^^\n",
      "        **kwargs\n",
      "        ^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\joeno\\anaconda3\\envs\\myenv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 135, in publish\n",
      "    msg = self.session.msg(msg_type, json_clean(content), parent=self.parent_header)\n",
      "                                                                 ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\joeno\\anaconda3\\envs\\myenv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 69, in parent_header\n",
      "    return self._parent_header.get()\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "LookupError: <ContextVar name='parent_header' at 0x0000021069C16D40>\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "if rank == 0:\n",
    "    #main_grid is the actual input input image array that is flattened\n",
    "    #convolution function arguments\n",
    "    #main_grid - data array (flattened array)\n",
    "    #kernel - kernel array\n",
    "    #DIMy - ColumnSize\n",
    "    #Dimx - RowSize\n",
    "    #upper_pad = upper padding row\n",
    "    #lower_pad = lower padding row\n",
    "    \n",
    "    #rename the below arguments according to your variable names\n",
    "    main_grid = img.flatten()\n",
    "    \n",
    "    #Entire convolution in a single process\n",
    "    conv1 = convolve_func(main_grid,kernel,KERNEL_DIM,DIMx,DIMy,upperPad,upperPad)\n",
    "    conv1 = np.reshape(conv1, (-1, DIMx))\n",
    "    #recvbuf is the convolution computed by parallel processes and gathered in process 0, \n",
    "    #if you named it different, modify that name below\n",
    "    \n",
    "    #Checking with parallel convolution output\n",
    "    print(np.array_equal(conv1, recvbuf))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myenv]",
   "language": "python",
   "name": "conda-env-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
